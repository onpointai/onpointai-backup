{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Deep Learning to Predict Pneumonia from X-Ray images\n",
    "> This notebook goes through the complete cycle of a typical machine learning project: data gathering, model development, training and prediction exercise using the Pytorch framework.\n",
    "\n",
    "- toc: true \n",
    "- sticky_rank: 2\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jovian, pytorch, fastpages, jupyter]\n",
    "- image: images/xray-pneumonia-1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This blog is towards the [Course Project](https://jovian.ml/forum/t/assignment-5-course-project/1563) for the [Pytorch Zero to GANS] free online course(https://jovian.ml/forum/c/pytorch-zero-to-gans/18) run by [JOVIAN.ML](https://www.jovian.ml).\n",
    "\n",
    "The course [competition](https://jovian.ml/forum/t/assignment-4-in-class-data-science-competition/1564/2) was based on analysing protein cells with muti-label classification.\n",
    "\n",
    "Therefore, to extend my understanding of dealing with medical imaging I decided to use the [X-Ray image database](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia) in Kaggle.\n",
    "\n",
    "Seeing as I ran out of GPU hours on Kaggle because of the competition (restricted to 30hrs/week at the time of writing June 2020) I opted to use Google Colab. \n",
    "\n",
    "This blog is in the form of a Jupyter notebook and inspired by [link](https://github.com/viritaromero/Detecting-Pneumonia-in-Chest-X-Rays/blob/master/Detecting_Pneumonia.ipynb).\n",
    "\n",
    "The blog talks about getting the dataset in Google Colab, explore the dataset, develop the training model, metrics and then does some preliminary training to get a model which is then used to make a few predictions. \n",
    "I will then talk about some of the lessons learned.\n",
    "\n",
    "> Warning! The purpose of this blog is to outline the steps taken in a typical Machine Learning project and should be treated as such.\n",
    "\n",
    "**Link to non-sanitised notebook on Jovian.ML here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1155,
     "status": "ok",
     "timestamp": 1592996787151,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "NXgIDphxMSLc"
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import PIL\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.transforms as T\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import OrderedDict\n",
    "from torchvision.utils import make_grid\n",
    "from torch.autograd import Variable\n",
    "import seaborn as sns\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab setup and getting data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJ4GxXV7-yiC"
   },
   "source": [
    "I used Google Colab with GPU processing for this project because I had exhausted my Kaggle hours (30hrs/wk) working on the competition :( The challenge here was signing into Colab, setting up the working directoty and then linking to Kaggle and copying the data over. The size of the dataset was about 1.3Gb which wasn't too much of a bother as Google gives each Gmail account 15Gb for free!\n",
    "\n",
    "\n",
    "> Tip: I used the monokai settings in Colab which gave excellent font contrast and colours for editing.![monokai](https://github.com/onpointai/onpointai/blob/master/images/xray-colab-monokai.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20518,
     "status": "ok",
     "timestamp": 1592991761241,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "hKw4rH7m-yiE",
    "outputId": "4bf9fa44-0847-4f2c-dab4-f69f1fd2d807"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default directory that is linked to the Google's gdrive ( the one connected to the gmail address) is\n",
    "/content/drive/My Drive/\n",
    "\n",
    "I create a project directory jovian-xray and use this as the new root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1592988101277,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "ksV82etl9lMC",
    "outputId": "e4b57186-b639-46f8-be63-599d092548ab"
   },
   "outputs": [],
   "source": [
    "os.listdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2892,
     "status": "ok",
     "timestamp": 1592991771798,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "MsWWRBYI9RT3",
    "outputId": "1e8823ea-80da-4f53-9bb0-b8f4a7835be4"
   },
   "outputs": [],
   "source": [
    "root_dir = '/content/drive/My Drive/jovian-xray'\n",
    "os.chdir(root_dir)\n",
    "!pwd\n",
    "os.mkdir('kaggle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Kaggle in your current Colab session.\n",
    "Log into Kaggle, point to the dataset and copy the API key. This downloads a kaggle.json file.\n",
    "Upload this kaggle.json to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVU-NN697OPd"
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RX7Rwtj_7P7d"
   },
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the kaggle.json file. This will be uploaded to your current working directory which is the root_dir as specified above.\n",
    "Create a ./kaggle directory  in the home directory\n",
    "Copy the kaggle.json from the current directory to this new directory.\n",
    "Change permissions so that it can be executed by user and group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11334,
     "status": "ok",
     "timestamp": 1592912181517,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "EbJE-0ip7PyK",
    "outputId": "1fba594c-a589-4f0f-dc8a-dea17fc362ec"
   },
   "outputs": [],
   "source": [
    "upload = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2553,
     "status": "ok",
     "timestamp": 1592912300507,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "EQgK0GsZ80KT",
    "outputId": "d2fc08b3-430b-463e-e5eb-2b7fab55e6b5"
   },
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4670,
     "status": "ok",
     "timestamp": 1592912323970,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "JhejeoAt-0jo",
    "outputId": "def44925-ea6c-4a93-eafc-f3ffe4690096"
   },
   "outputs": [],
   "source": [
    "!ls\n",
    "!cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utJPar1h-Sxy"
   },
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2693,
     "status": "ok",
     "timestamp": 1592991779838,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "Q5RRbcsd_Njj",
    "outputId": "1524b689-f0fb-45a9-c7a7-7a4682b7c3ba"
   },
   "outputs": [],
   "source": [
    "proj_dir = os.path.join(root_dir, 'kaggle', 'chest_xray')\n",
    "os.chdir(proj_dir)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [Kaggle data directory](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia) page select New Notebook > Three vertical dots, Copy API Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33672,
     "status": "ok",
     "timestamp": 1592912657341,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "cvGIyTKW_G4k",
    "outputId": "04da74a7-af18-49ae-9ac9-e231ea1bebc4"
   },
   "outputs": [],
   "source": [
    "#API key\n",
    "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2qvVoFVC_rlQ"
   },
   "outputs": [],
   "source": [
    "!unzip chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 982,
     "status": "ok",
     "timestamp": 1592921954662,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "1mDu3HYJ_rg6",
    "outputId": "a4635458-b29f-4828-f730-1bd7e3937e31"
   },
   "outputs": [],
   "source": [
    "os.listdir(proj_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is structured into training, val and test folders, each with sub-folders of NORMAL and PNEUMONIA images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image transforms\n",
    "\n",
    "We will now prepare the data for reading into Pytorch as numpy arrays using DataLoaders.\n",
    "\n",
    "Havig data augmentation is a good way to get extra training data for free. However, care must be taken to ensure that the transforms requested are likely to appear in the inference (or test set).\n",
    "\n",
    "The images (RGB) are normalized using the mean [0.485,0.456,0.406] and standard deviation [0.229,0.224,0.225] of that used for the Imagenet data in the Resnet model, so that the new input images have the same distribution and mean as that used in the Resnet model.\n",
    "\n",
    "I have set up two transforms dictionaries, one with and one without so it would be easy to plot images and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1592992969806,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "8edp-RxrFUAE"
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {'train' : T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "    T.RandomRotation(20),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*imagenet_stats, inplace=True)\n",
    "]),\n",
    "'test' : T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*imagenet_stats, inplace=True)\n",
    "]),\n",
    "'val' : T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*imagenet_stats, inplace=True)\n",
    "])}\n",
    "\n",
    "data_no_transforms = {'train' : T.Compose([ T.ToTensor() ]),\n",
    "'test' : T.Compose([T.ToTensor() ]),\n",
    "'val' : T.Compose([T.ToTensor() ])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "Set up the project directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1592991785343,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "W5aK5zm-Q6lK",
    "outputId": "1bfbb373-91da-4844-f95c-00b31da278dd"
   },
   "outputs": [],
   "source": [
    "proj_dir = os.path.join(root_dir, 'kaggle', 'chest_xray')\n",
    "\n",
    "proj_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "op: '/content/drive/My Drive/jovian-xray/kaggle/chest_xray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1285,
     "status": "ok",
     "timestamp": 1592996722840,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "yFd3MmZFw5Ac"
   },
   "outputs": [],
   "source": [
    "image_datasets = {x: datasets.ImageFolder(os.path.join(proj_dir, x),\n",
    "                                          data_transforms[x]) for x in ['train', 'val','test']}\n",
    "\n",
    "# Using the image datasets and the transforms, define the dataloaders\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=2) for x in ['train', 'val','test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "class_names= image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "op: ['NORMAL', 'PNEUMONIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 972,
     "status": "ok",
     "timestamp": 1592993422011,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "VSR1m-88zNgi",
    "outputId": "3b2ddd43-18e4-458f-9ad6-75abdeff16ae"
   },
   "outputs": [],
   "source": [
    "# Processed Images\n",
    "print(image_datasets['train'][0][0].shape)\n",
    "print(image_datasets['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "op: torch.Size([3, 224, 224])  \n",
    "Dataset ImageFolder  \n",
    "    Number of datapoints: 5216  \n",
    "    Root location: /content/drive/My Drive/jovian-xray/kaggle/chest_xray/train  \n",
    "    StandardTransform  \n",
    "Transform: Compose(  \n",
    "               Resize(size=224, interpolation=PIL.Image.BILINEAR)  \n",
    "               CenterCrop(size=(224, 224))  \n",
    "               RandomRotation(degrees=(-20, 20), resample=False, expand=False)  \n",
    "               RandomHorizontalFlip(p=0.5)  \n",
    "               ToTensor()  \n",
    "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation\n",
    "\n",
    "The image data is converted in to Numpy arrays and then treated with the mean and std so that we can view the images as seen by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, title=None):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "def raw_imshow(img, title=None):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "images, classes = next(iter(dataloaders['train']))\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(images)\n",
    "plt.figure(figsize=(8, 8))\n",
    "raw_imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![xray-raw-images](https://github.com/onpointai/onpointai/blob/master/images/xray-raw-images.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "images, classes = next(iter(dataloaders['train']))\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(images)\n",
    "plt.figure(figsize=(8, 8))\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![xray-processed-images](https://github.com/onpointai/onpointai/blob/master/images/xray-processed-images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MaLbpoy4DX9A"
   },
   "source": [
    "## Use a Resnet34 model with our custom classifier for X-ray pneumonia images\n",
    "\n",
    "The method of transfer learning is widely used to take advantage of the clever and hardworking chaps who have spent time to train a model on million+ images and save the trained model architecture and weights.\n",
    "\n",
    "The Resnet34 model has been trained on the Imagenet database which has 1000 classes [from trombones to toilet tissue.](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)\n",
    "\n",
    "Resnet34 has a top-most fully connected layer to predict 1000 classes. In our case we need only two so we will remove the last fc layer and add our own.\n",
    "\n",
    "Have a look [here](https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8) for a good explanation of Resnet architectures. Briefly, the after each set of convolutions the input is added to the output. This helps to maintain the reslotion of the input, ie do not lose any features of the input model.\n",
    "\n",
    "In a deep neural network the early layers capture generic features such as edges, texture  and colour while the latter layers capture more specific features such as cats ears, eyes, elephant trunks and so on.\n",
    "\n",
    "So our process is take the trained resnet architecture and weights, remove the head ie the last layers that are used to predict the 1000 classes and add our own tailored to the number of classes we want to predict, which in our case is two.\n",
    "\n",
    "We will do a first pass of training where the weights of the resnet model are locked ie, ie we do not want to overwrite or lose those values which will mean more GPU expense for us. Then we will unfreeze the weights and run the entire model at our prefereed laerning rate. Note, idelaly we would like to unfreeze only specific layer, say layer 1 and layer 4, which I will cover in a separate blogpost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "be171307fcc34e148e89819217a4d421",
      "22ac3b599ee447b79775fff9cbd343b9",
      "8df1fbc272d8452fab11f3847e046458",
      "0a3a7e4ae40c4b4fae745c03953348d0",
      "10f580b1ed08477487aaf0e8d7184b2d",
      "720c7b02581c4a969a7b7bdef57a1c0e",
      "bad4d3faa74549809bc9192525a19854",
      "03f8cbf801e04e85b287ff06692ce3eb"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3702,
     "status": "ok",
     "timestamp": 1592993462565,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "4ejK_LQFEnBq",
    "outputId": "66317949-10b6-4dcd-b1ff-4bb11b48a610"
   },
   "outputs": [],
   "source": [
    "# Build and train your network\n",
    "# 1. Load resnet-34 pre-trained network\n",
    "model = models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tailed output of the model summary gives:\n",
    "```\n",
    "  )\n",
    "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
    ")\n",
    "```\n",
    "In the Resnet34 architecture the final fully connected layer has\n",
    " in_features and 1000 out_features for the 1000 classes. But we need only two output classes.\n",
    "\n",
    "So we add two linear layers to go from 512 RELU 256 and then 256 LOGSOFTMAX to 2 classes\n",
    "\n",
    "Use a [LogSoftmax](https://pytorch.org/docs/master/generated/torch.nn.LogSoftmax.html) for the final classification activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1592993464612,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "5R312ah2Em-O"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(2048, 1024)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(1024, 2)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "\n",
    "# Replacing the pretrained model classifier with our classifier\n",
    "model.fc = classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up freeze and unfreeze subs to freeze resnet34  model layers before prelim training and then unfreeze model for final training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(model):\n",
    "  # To freeze the residual layers\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "  for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "def unfreeze(self):\n",
    "  # Unfreeze all layers\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (512 * 256 + 256) + ( 256 *2 +2)\n",
    "cp = count_parameters(model)\n",
    "print(f'{cp} trainable parameters in frozen model  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "op: 131842 trainable parameters in frozen model  \n",
    "\n",
    "Check: (512 * 256 + 256 bias) + (256 * 2 + 2 bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the Training (and Validation) model\n",
    "\n",
    "The dataset has training, val and tests which makes our lives a little bot easier ie we don't have to do any data splitting and can set up specific transforms for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1626,
     "status": "ok",
     "timestamp": 1592995356478,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "OA89R06lEm59"
   },
   "outputs": [],
   "source": [
    "#Training the model\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best valid accuracy: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1326,
     "status": "ok",
     "timestamp": 1592995360242,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "OFNkvohnu0T_",
    "outputId": "7a33ce95-d11e-4190-cffe-546ed6a2284a"
   },
   "outputs": [],
   "source": [
    "nThreads = 4\n",
    "batch_size = 32\n",
    "use_gpu = torch.cuda.is_available()\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Training (and Validation) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [Adam optimizer](https://pytorch.org/docs/stable/optim.html#) which is the preferred optimizer because it is adaptive and adds a momentum element to the gradient stepping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1315,
     "status": "ok",
     "timestamp": 1592995364173,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "id2UbJoIupUc",
    "outputId": "b8d6493b-ecf9-4d38-cbc4-543a67a636db"
   },
   "outputs": [],
   "source": [
    "#Train a model with a pre-trained network\n",
    "num_epochs = 10\n",
    "if use_gpu:\n",
    "    print (\"Using GPU: \"+ str(use_gpu))\n",
    "    model = model.cuda()\n",
    "\n",
    "# NLLLoss because our output is LogSoftmax\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#Adam optimizer with a learning rate\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.0001)\n",
    "#optimizer = optim.SGD(model.fc.parameters(), lr = .1, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 5 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1001724,
     "status": "ok",
     "timestamp": 1592996378589,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "wrIfuLHeupRZ",
    "outputId": "51ca64bf-c334-4bd8-d0a6-231f08fa2a6a"
   },
   "outputs": [],
   "source": [
    "model_fit = train_model(model, criterion, optimizer(lr=0.001), exp_lr_scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfreeze the model and train some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = count_parameters(model)\n",
    "print(f'{cp} trainable parameters in unfrozen model  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "op: 21416514 trainable parameters in unfrozen model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = train_model(model, criterion, optimizer(lr=0.0001), exp_lr_scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1592996660313,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "U1Tmwya-upLr"
   },
   "outputs": [],
   "source": [
    "# Do validation on the test set\n",
    "def test(model, dataloaders, device):\n",
    "  model.eval()\n",
    "  accuracy = 0\n",
    "  \n",
    "  model.to(device)\n",
    "    \n",
    "  for images, labels in dataloaders['test']:\n",
    "    images = Variable(images)\n",
    "    labels = Variable(labels)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "      \n",
    "    output = model.forward(images)\n",
    "    ps = torch.exp(output)\n",
    "    equality = (labels.data == ps.max(1)[1])\n",
    "    accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "      \n",
    "    print(\"Testing Accuracy: {:.3f}\".format(accuracy/len(dataloaders['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 151824,
     "status": "ok",
     "timestamp": 1592996943838,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "H0EfMYmwupIz",
    "outputId": "a08bb145-0038-496c-9f35-f2054ac8882a"
   },
   "outputs": [],
   "source": [
    "test(model, dataloaders, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the interim model (checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1687,
     "status": "ok",
     "timestamp": 1592997063638,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "2WlyYUa7BO52"
   },
   "outputs": [],
   "source": [
    "# Save the checkpoint \n",
    "model.class_to_idx = dataloaders['train'].dataset.class_to_idx\n",
    "model.epochs = num_epochs\n",
    "checkpoint = {'input_size': [2, 224, 224],\n",
    "                 'batch_size': dataloaders['train'].batch_size,\n",
    "                  'output_size':2,\n",
    "                  'state_dict': model.state_dict(),\n",
    "                  'data_transforms': data_transforms,\n",
    "                  'optimizer_dict':optimizer.state_dict(),\n",
    "                  'class_to_idx': model.class_to_idx,\n",
    "                  'epoch': model.epochs}\n",
    "save_fname= os.path.join(proj_dir, '90_checkpoint.pth')\n",
    "torch.save(checkpoint,  save_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that loads a checkpoint and rebuilds the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2601,
     "status": "ok",
     "timestamp": 1592997078805,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "bB7oNHpnupGC"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = models.resnet152()\n",
    "    \n",
    "    # our input_size matches the in_features of pretrained model\n",
    "    input_size = 2048\n",
    "    output_size = 2\n",
    "    \n",
    "    classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(2048, 1024)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          #('dropout1', nn.Dropout(p=0.2)),\n",
    "                          ('fc2', nn.Linear(1024, 2)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "\n",
    "\n",
    "    # Replacing the pretrained model classifier with our classifier\n",
    "    model.fc = classifier\n",
    "    \n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model, checkpoint['class_to_idx']\n",
    "\n",
    "# Get index to class mapping\n",
    "loaded_model, class_to_idx = load_checkpoint(save_fname)\n",
    "idx_to_class = { v : k for k,v in class_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the Training/Validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 972,
     "status": "ok",
     "timestamp": 1592997088113,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "P2iOP-0HupCr"
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1638,
     "status": "ok",
     "timestamp": 1592997098344,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "z2tC6K0huo-O",
    "outputId": "5a47a5f4-afa6-4ac6-8246-4e31cdf3941a"
   },
   "outputs": [],
   "source": [
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![xray-visualise-model](https://github.com/onpointai/onpointai/blob/master/images/xray-visualise-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 954,
     "status": "ok",
     "timestamp": 1592997498642,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "bUgschSeCB1A"
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    \n",
    "    # Process a PIL image for use in a PyTorch model\n",
    "    #T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    \n",
    "    size = 256, 256\n",
    "    image.thumbnail(size, Image.ANTIALIAS)\n",
    "    image = image.crop((128 - 112, 128 - 112, 128 + 112, 128 + 112))\n",
    "    npImage = np.array(image)\n",
    "    npImage = npImage/255.\n",
    "        \n",
    "    imgA = npImage[:,:,0]\n",
    "    imgB = npImage[:,:,1]\n",
    "    imgC = npImage[:,:,2]\n",
    "    \n",
    "    imgA = (imgA - 0.485)/(0.229) \n",
    "    imgB = (imgB - 0.456)/(0.224)\n",
    "    imgC = (imgC - 0.406)/(0.225)\n",
    "        \n",
    "    npImage[:,:,0] = imgA\n",
    "    npImage[:,:,1] = imgB\n",
    "    npImage[:,:,2] = imgC\n",
    "    \n",
    "    npImage = np.transpose(npImage, (2,0,1))\n",
    "    \n",
    "    return npImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 888,
     "status": "ok",
     "timestamp": 1592997513905,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "m5L-zLn1CByM"
   },
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.numpy().transpose((0, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 892,
     "status": "ok",
     "timestamp": 1592997230640,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "HMsrMVcUCBs5"
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "def predict(image_path, model, topk=2):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    \n",
    "    # Implement the code to predict the class from an image file\n",
    "    \n",
    "    image = torch.FloatTensor([process_image(Image.open(image_path).convert('RGB'))])\n",
    "    model.eval()\n",
    "    output = model.forward(Variable(image))\n",
    "    probabilities = torch.exp(output).data.numpy()[0]\n",
    "    \n",
    "\n",
    "    top_idx = np.argsort(probabilities)[-topk:][::-1] \n",
    "    top_class = [idx_to_class[x] for x in top_idx]\n",
    "    top_probability = probabilities[top_idx]\n",
    "\n",
    "    return top_probability, top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1610,
     "status": "ok",
     "timestamp": 1592997705867,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "upiD5EbyCBqL",
    "outputId": "1eac216b-9c5f-4915-8628-2510ba8afb55"
   },
   "outputs": [],
   "source": [
    "print (predict('chest_xray/test/NORMAL/NORMAL2-IM-0348-0001.jpeg', loaded_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (array([0.96704894, 0.03295105], dtype=float32), ['NORMAL', 'PNEUMONIA'])\n",
    " \n",
    " The NORMAL probablility is much larger than the one for pneumonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 883,
     "status": "ok",
     "timestamp": 1592997714051,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "ZuS7n3PZCBnB"
   },
   "outputs": [],
   "source": [
    "# Display an image along with the top  classes\n",
    "def view_classify(img, probabilities, classes, mapper):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    img_filename = 'Prediction'\n",
    "    img = Image.open(img)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,10),  ncols=1, nrows=2)\n",
    "    ct_name = img_filename\n",
    "    \n",
    "    ax1.set_title(ct_name)\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    y_pos = np.arange(len(probabilities))\n",
    "    g = sns.barplot(x =y_pos, y=probabilities, palette='Blues')\n",
    "    for index, row in enumerate(probabilities):\n",
    "      g.text(index, row, row, color='fuchsia', ha= 'center')\n",
    "    ax2.set_xticklabels(x for x in classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1785,
     "status": "ok",
     "timestamp": 1592997717064,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "bRCPzE2cCBj8",
    "outputId": "4930e05e-4682-421d-edb7-6e77b1878cbf"
   },
   "outputs": [],
   "source": [
    "img = os.path.join(proj_dir, 'test/NORMAL/NORMAL2-IM-0347-0001.jpeg')\n",
    "p, c = predict(img, loaded_model)\n",
    "view_classify(img, p, c, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Prediction:](https://github.com/onpointai/onpointai/blob/master/images/xray-predict-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1508,
     "status": "ok",
     "timestamp": 1592997835834,
     "user": {
      "displayName": "Dexter D'Silva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhW_MLYF6gRb-aYgCSu3HZ63AR8fiI4Z9lPWaPL=s64",
      "userId": "14942342950933263184"
     },
     "user_tz": -60
    },
    "id": "5FomP4g4CiI0",
    "outputId": "3f3166b8-183e-4480-9e20-ecdd8c5744ef"
   },
   "outputs": [],
   "source": [
    "img = os.path.join(proj_dir, 'test/PNEUMONIA/person85_bacteria_421.jpeg')\n",
    "p, c = predict(img, loaded_model)\n",
    "view_classify(img, p, c, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: -c: line 0: syntax error near unexpected token `https://github.com/onpointai/onpointai/blob/master/images/xray-predict-2.png'\r\n",
      "/bin/sh: -c: line 0: `[Prediction:](https://github.com/onpointai/onpointai/blob/master/images/xray-predict-2.png)'\r\n"
     ]
    }
   ],
   "source": [
    "![Prediction:](https://github.com/onpointai/onpointai/blob/master/images/xray-predict-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are five methods to reduce model overfitting.  Overfitting results when the model fits very well to the training data (low error) but not very well to the validation data (high error).\n",
    "These are:\n",
    "> Get more data  \n",
    "> Data augmentation  \n",
    "> Generalizable architectures  \n",
    "> Regularisation  \n",
    "> Reduce architecture complexity  \n",
    "\n",
    "2. Undertaking an online course like the [Jovian Zero to Gans]((https://jovian.ml/forum/c/pytorch-zero-to-gans/18)) has been an excellent opportunity to immerse myself in Machine Learning. Taking part in the competition (which is ongoing) and writing this blog on the X-ray dataset has helped me to better understand important concepts such as Dataloaders, learning rate, batch size, optimizers and loss functions.    \n",
    "\n",
    "\n",
    "3. Thank you to Aakash the course instructor and the rest pf the Jovian team for the efforts in helping us to better understand such an exciting paradigm.\n",
    "\n",
    "------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "jovian-xray-mthd-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03f8cbf801e04e85b287ff06692ce3eb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a3a7e4ae40c4b4fae745c03953348d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03f8cbf801e04e85b287ff06692ce3eb",
      "placeholder": "",
      "style": "IPY_MODEL_bad4d3faa74549809bc9192525a19854",
      "value": " 230M/230M [00:01&lt;00:00, 154MB/s]"
     }
    },
    "10f580b1ed08477487aaf0e8d7184b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "22ac3b599ee447b79775fff9cbd343b9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "720c7b02581c4a969a7b7bdef57a1c0e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8df1fbc272d8452fab11f3847e046458": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_720c7b02581c4a969a7b7bdef57a1c0e",
      "max": 241530880,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_10f580b1ed08477487aaf0e8d7184b2d",
      "value": 241530880
     }
    },
    "bad4d3faa74549809bc9192525a19854": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be171307fcc34e148e89819217a4d421": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8df1fbc272d8452fab11f3847e046458",
       "IPY_MODEL_0a3a7e4ae40c4b4fae745c03953348d0"
      ],
      "layout": "IPY_MODEL_22ac3b599ee447b79775fff9cbd343b9"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
