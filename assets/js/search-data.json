{
  
    
        "post0": {
            "title": "Loss Functions",
            "content": "About . A Loss Function is a function that calculates a single real number which indicates how far a prediction is from the actual. During the creation of a Machine Learning model the loss is minimised by using some type of algorithm also known as an Optimiser. The Optimiser optimises the result by reducing the loss. In general, the loss is calculated over a number of input examples (batch). . Tutorial Overiew . JASON BROWNLEE MACHINE LEARNING MASTERY . This tutorial is divided into three parts; they are: . Regression Loss Functions . Mean Squared Error Loss - default - distribution of target is gaussian - square of error means larger errors penalised more than smaller ones (Keras mse or mean_squared_error) . Mean Squared Logarithmic Error Loss . Mean Absolute Error Loss - distribution of target variable is mainly gaussian but may heave outliers (keras - mean_absolute_error) . Binary Classification Loss Functions - targets are either of two labels . **Binary Cross-Entropy (keras - binary_cross_entropy) . Hinge Loss - for use with SVM - binay classification where target values are in the set {-1,1} encourages examples to have the correct sign - assigning more errorwhen thee is a differene in sign between actual/predicted class values . Squared Hinge Loss - square of hinge loss - hass effect of smoothing the surfaceof the error function and making it easier to work with numerically . Multi-Class Classification Loss Functions - targets can belong to one of many labels or classes - predict probability of example beloging to each known class . Multi-Class Cross-Entropy Loss - use this first - calculate a score that summarizes the ave diff between actual and predicted probability distributions for all classes - keras -categorical_cross_entropy . Sparse Multiclass Cross-Entropy Loss - large nb of labels- eg words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. No one-hot encoding keras - sparse_categorical_crossentropy . Kullback Leibler Divergence Loss KL divergence - measure of how one probability distribution differs from a baseline distribution KLdiv of 0 suggest distributions are identical calculates how much information is lost if the predicted target distributon is use to approximate the desired target probablity distribuion KLd more commonly used when using models that learn to approximate a more complex fn than simply multi-class ie autoencoding used for learning a dense feature representation under a model that must construct the original input . Regression loss functions . #&#39;/Users/dexterdsilva/Documents/Developer/MachineLearning/brownlee_mlm&#39; import os os.path.abspath(&#39;&#39;) . &#39;/Users/dexterdsilva/Documents/Developer/MachineLearning/brownlee_mlm&#39; . from sklearn.datasets import make_regression from sklearn.preprocessing import StandardScaler from keras.models import Sequential from keras.layers import Dense from keras.optimizers import SGD import matplotlib.pyplot as plt . Using TensorFlow backend. . import keras print(keras.__version__) import tensorflow as tf print(tf.__version__) . 2.3.1 2.1.0 . #generate regression dataset #20 X,y=make_regression(n_samples=1000, n_features=20,noise=0.1, random_state=1) . X[0] . array([ 0.58372668, 0.78593639, -0.17187155, 0.66928708, 1.67181016, 0.59831823, 1.49807611, 0.27925069, -0.31705821, -0.41961259, -0.21796143, 0.81186707, -0.79215259, 0.56621046, 0.97473625, -0.8223744 , 1.03007179, -0.67945508, -0.21540618, 1.03118947]) . print(y.shape) f&#39;{y[0]}&#39; print(type(y)) . (1000,) &lt;class &#39;numpy.ndarray&#39;&gt; . X=StandardScaler().fit_transform(X) . X[0] . array([ 0.64516745, 0.76000873, -0.18010938, 0.65740427, 1.65197553, 0.56574009, 1.50368462, 0.30416606, -0.29634854, -0.37041198, -0.22745535, 0.72392625, -0.76421488, 0.55675061, 0.96630152, -0.82406123, 0.95038766, -0.76479647, -0.2088517 , 1.04702956]) . y=StandardScaler().fit_transform(y.reshape(len(y),1))[:,0] . n_train=500 trainX, testX = X[:n_train,:], X[n_train:,:] trainy, testy = y[:n_train], y[n_train:] . model= Sequential() model.add(Dense(25, input_dim=20,activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(1, activation=&#39;linear&#39;)) . opt=SGD(lr=0.01, momentum=0.9) . Mean Squared Error Loss . model.compile(loss=&#39;mse&#39;, optimizer = opt) . h=model.fit(trainX, trainy, validation_data=(testX,testy), epochs=100, verbose=0) . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;loss&#39;]) . fig= plt.figure(figsize=(5,5)) plt.plot(h.history[&#39;loss&#39;],label=&#39;train_loss&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test_loss&#39;) plt.legend() plt.show() . Mean Squared Logarithmic Error Loss . model=Sequential() model.add(Dense(25, input_dim=20, activation =&#39;relu&#39;, kernel_initializer = &#39;he_uniform&#39;)) model.add(Dense(1, activation = &#39;linear&#39;)) opt=SGD(lr=0.1, momentum=0.9) model.compile(loss=&#39;mean_squared_logarithmic_error&#39;, optimizer=opt, metrics=[&#39;mse&#39;]) . print(trainX.shape,&#39; &#39;,trainy.shape,&#39; &#39;, testX.shape,&#39; &#39;, testy.shape) . (500, 20) (500,) (500, 20) (500,) . h = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0) . # evaluate the model _,train_mse=model.evaluate(trainX, trainy, verbose=0) _,test_mse=model.evaluate(testX, testy, verbose=0) . print(&#39;Train: {:.2f} Test {:.2f}&#39;.format(train_mse, test_mse)) . Train: 0.31 Test 0.37 . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_mse&#39;, &#39;loss&#39;, &#39;mse&#39;]) . fig=plt.figure(figsize=(10,10)) plt.subplot(211) plt.title(&#39;Loss&#39;) plt.plot(h.history[&#39;loss&#39;],label=&#39;Training Loss&#39;) plt.plot(h.history[&#39;val_loss&#39;],label = &#39;Test Loss&#39;) plt.legend() plt.subplot(212) plt.title(&#39;Mean Squared Error&#39;) plt.plot(h.history[&#39;mse&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_mse&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Mean Absolute Error Loss . X,y=make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1) . print(X.shape, y.shape,&#39; &#39;, type(y)) plt.scatter(X[:,1],y) . (1000, 20) (1000,) &lt;class &#39;numpy.ndarray&#39;&gt; . &lt;matplotlib.collections.PathCollection at 0x13e0af090&gt; . len(y) . 1000 . X=StandardScaler().fit_transform(X) y=StandardScaler().fit_transform(y.reshape(len(y),1)) . n_train=500 trainX, testX = X[:n_train,:],X[n_train:,:] trainy, testy = y[:n_train], y[n_train:] . model= Sequential() model.add(Dense(25,input_dim=20, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(1, activation=&#39;linear&#39;)) opt=SGD(lr=0.01, momentum=0.9) model.compile(loss=&#39;mean_absolute_error&#39;, optimizer=opt,metrics=[&#39;mse&#39;]) . h=model.fit(trainX, trainy,validation_data=(testX, testy), epochs=100, verbose=1) . _,train_mse=model.evaluate(trainX, trainy, verbose=1) . 500/500 [==============================] - 0s 18us/step . _,test_mse=model.evaluate(testX, testy, verbose=1) . 500/500 [==============================] - 0s 19us/step . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_mse&#39;, &#39;loss&#39;, &#39;mse&#39;]) . print(&#39;Train: {:.4f} Test:{:.4f}&#39;.format(train_mse, test_mse)) . Train: 0.0042 Test:0.0036 . fig=plt.figure(figsize=(10,10)) plt.subplot(121) plt.title(&#39;LOSS&#39;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&#39;MSE&#39;) plt.plot(h.history[&#39;mse&#39;],label=&#39;train&#39;) plt.plot(h.history[&#39;val_mse&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Classification Loss Functions . from sklearn.datasets import make_circles from sklearn.preprocessing import StandardScaler from keras.models import Sequential from keras.layers import Dense from keras.optimizers import SGD from numpy import where import matplotlib.pyplot as plt %matplotlib inline . X,y = make_circles(n_samples=1000,noise=0.1, random_state=1) for i in range(2): samples_idx= where(y==i) plt.scatter(X[samples_idx,0], X[samples_idx,1], label=str(i)) plt.legend() plt.show() . print(X[:5]) print(y[:5]) . [[ 0.92787748 -0.04521731] [-0.54303182 -0.75444674] [ 0.9246533 -0.71492522] [-0.10217077 -0.89283523] [-1.01719242 0.24737775]] [1 1 0 0 0] . n_train=500 trainX,testX=X[:n_train,:],X[:n_train] trainy,testy = y[:n_train], y[:n_train] . print(trainX.shape ,&#39; &#39;, trainy.shape) . (500, 2) (500,) . Binary Cross Entropy . model=Sequential() model.add(Dense(50, input_dim=2, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) opt=SGD(lr=0.01, momentum=0.9) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;]) . h=model.fit(trainX, trainy, validation_data=(testX,testy), epochs=200,verbose=1) . #_,train_acc=model.evaluate(trainX,trainy, verbose=1) . 500/500 [==============================] - 0s 44us/step . _,test_acc=model.evaluate(testX, testy,verbose=1) . 500/500 [==============================] - 0s 25us/step . print(&#39;Train: {:.4f} Test:{:.4f}&#39;.format(train_acc, test_acc)) . Train: 0.8360 Test:0.8360 . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . fig=plt.figure(figsize=(8,4)) plt.subplot(121) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&quot;ACCURACY&quot;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Hinge Loss . y[where(y==0)]= -1 . X,y = make_circles(n_samples=1000, noise=0.1, random_state=1) y[where(y==0)]= -1 . n_train=500 trainX,testX= X[:n_train,:],X[n_train:,:] trainy,testy=y[:n_train],y[n_train:] . model=Sequential() model.add(Dense(50,input_dim=2,activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(1, activation=&#39;tanh&#39;)) opt=SGD(lr=0.01, momentum=0.9) model.compile(loss=&#39;hinge&#39;, optimizer=opt,metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential_14&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_24 (Dense) (None, 50) 150 _________________________________________________________________ dense_25 (Dense) (None, 1) 51 ================================================================= Total params: 201 Trainable params: 201 Non-trainable params: 0 _________________________________________________________________ . h=model.fit(trainX,trainy,validation_data=(testX,testy), epochs=200, verbose=1) . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . _,train_acc=model.evaluate(trainX,trainy, verbose=1) . 500/500 [==============================] - 0s 21us/step . _,test_acc=model.evaluate(testX,testy, verbose= 1) . 500/500 [==============================] - 0s 19us/step . print(&#39;Train: {:.4f} Test: {:.4f}&#39;.format(train_acc, test_acc)) . Train: 0.8380 Test: 0.8380 . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . fig =plt.figure(figsize=(10,5)) plt.subplot(1,2,1) plt.title(&#39;ACCURACY&#39;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label = &#39;test&#39;) plt.legend() plt.subplot(1,2,2) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Squared Hinge Loss . X,y = make_circles(n_samples=1000, noise=0.1, random_state=1) y[where(y==0)]= -1 . n_train=500 trainX,testX = X[:n_train,:],X[n_train:,:] trainy,testy=y[:n_train],y[n_train:] . collapse-hide model = Sequential() model.add(Dense(50, input_dim=2, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(1, activation=&#39;tanh&#39;)) opt=SGD(lr=0.01, momentum=0.9) model.compile(loss=&#39;squared_hinge&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;]) h = model.fit(trainX,trainy,validation_data=(testX,testy), epochs=200, verbose=1) . print(model.metrics_names) res_train = model.evaluate(trainX,trainy) res_test = model.evaluate(testX, testy) . [&#39;loss&#39;, &#39;accuracy&#39;] 500/500 [==============================] - 0s 18us/step 500/500 [==============================] - 0s 18us/step . print(&#39;Traina cc:{:.4f} Test acc:{:.4f}&#39;.format(res_train[1], res_test[1])) . Traina cc:0.6820 Test acc:0.6660 . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . fig=plt.figure(figsize=(8,2)) plt.subplot(121) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&quot;ACCURACY&quot;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Multi-class classification . Multi-class cross-entropy . from sklearn.datasets import make_blobs X,y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2) . print(X[:5], &#39; &#39;, y[:5]) . [[ 0.48719811 -0.43160548] [ -1.48958879 -3.47915742] [ -2.06250444 -7.73300419] [ -0.51369303 -10.31546366] [ 0.56240126 -2.18246169]] [2 2 2 0 1] . for i in range(3): samples_idx = where(y==i) plt.scatter(X[samples_idx,0], X[samples_idx,1], label=str(i)) plt.legend() plt.show() . from keras.utils import to_categorical X,y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2) y = to_categorical(y) #NO ONEHOT ENCODING FOR SPARSE n_train=500 trainX,testX = X[:n_train,:],X[n_train:,:] trainy,testy=y[:n_train],y[n_train:] . print(trainy.shape) print(y[:5]) . (500, 3) [[0. 0. 1.] [0. 0. 1.] [0. 0. 1.] [1. 0. 0.] [0. 1. 0.]] . model = Sequential() model.add(Dense(50, input_dim=2, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(3, activation=&#39;softmax&#39;)) opt=SGD(lr=0.01, momentum=0.9) #model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;]) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;], ) h = model.fit(trainX,trainy,validation_data=(testX,testy), epochs=200, verbose=1) . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . res_train = model.evaluate(trainX,trainy) res_test = model.evaluate(testX, testy) print(&#39;Train acc:{:.4f} Test acc:{:.4f}&#39;.format(res_train[1], res_test[1])) . 500/500 [==============================] - 0s 42us/step 500/500 [==============================] - 0s 33us/step Train acc:0.8320 Test acc:0.8260 . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . #collapse-hide fig=plt.figure(figsize=(8,2)) plt.subplot(121) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&quot;ACCURACY&quot;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label=&#39;test&#39;) plt.legend() plt.show() . . Sparse multi-class cross-entropy . from keras.utils import to_categorical X,y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2) y = to_categorical(y) NO ONEHOT ENCODING FOR SPARSE n_train=500 trainX,testX = X[:n_train,:],X[n_train:,:] trainy,testy=y[:n_train],y[n_train:] print(trainy.shape) print(y[:5]) . (500,) [2 2 2 0 1] . model = Sequential() model.add(Dense(50, input_dim=2, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(3, activation=&#39;softmax&#39;)) opt=SGD(lr=0.01, momentum=0.9) #model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;]) model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;], ) h = model.fit(trainX,trainy,validation_data=(testX,testy), epochs=200, verbose=1) #res_train = model.evaluate(trainX,trainy) #res_test = model.evaluate(testX, testy) #print(&#39;Train acc:{:.4f} Test acc:{:.4f}&#39;.format(res_train[1], res_test[1])) . fig=plt.figure(figsize=(8,2)) plt.subplot(121) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&quot;ACCURACY&quot;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Kullback Leibler Divergence . Measure of how one probablity distribution differs form another KL Div = 0 suggest dist are identical autoenoder used for learning dense featue representation under a model that must reconstruct the original input . #collapse-hide from sklearn.datasets import make_blobs from keras.layers import Dense from keras.models import Sequential from keras.optimizers import SGD from keras.utils import to_categorical import matplotlib.pyplot as plt . . X,y = make_blobs(n_samples=1000,centers=3,n_features=2, cluster_std=2, random_state=2) . y[:5] . array([2, 2, 2, 0, 1]) . y = to_categorical(y) . y[:5] . array([[0., 0., 1.], [0., 0., 1.], [0., 0., 1.], [1., 0., 0.], [0., 1., 0.]], dtype=float32) . n_train=500 trainX,testX = X[:n_train,:],X[n_train:,:] trainy, testy = y[n_train:],y[:n_train] . #collapse-hide model= Sequential() model.add(Dense(50, input_dim=2, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(3, activation=&#39;softmax&#39;)) opt=SGD(lr=0.01, momentum=0.9) model.compile(loss=&#39;kullback_leibler_divergence&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;]) . . h = model.fit(trainX ,trainy, validation_data=(testX, testy), epochs =100, verbose=1) . model.evaluate(trainX,trainy,verbose=1) . 500/500 [==============================] - 0s 49us/step . [1.0587191171646118, 0.41600000858306885] . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . fig=plt.figure(figsize=(8,2)) plt.subplot(121) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&quot;ACCURACY&quot;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label=&#39;test&#39;) plt.legend() plt.show() . FIN .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/jupyter/2020/06/05/lossfunctions.html",
            "relUrl": "/ai/ml/jupyter/2020/06/05/lossfunctions.html",
            "date": " • Jun 5, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Sports Video Classification",
            "content": "Overview . The simplest way to classify the type of video is by examining individual frames of the video ie treat individual frames as images, classify the images and then do some sort of averaging or smoothing over a few frames to predict the current display. . Once we understand what goes on here we can explore the other methods which take up more setup and compute. . The main package here is OpenCV, a library aimed at real-time computer vision tasks. . The dataset used was a collection of images curated using Google image search. . ├── Sports-Type-Classifier │ ├── data │ │ ├── badminton [938 entries] │ │ ├── baseball [746 entries] │ │ ├── basketball [495 entries] │ │ ├── boxing [705 entries] │ │ ├── chess [481 entries] │ │ ├── cricket [715 entries] │ │ ├── fencing [635 entries] │ │ ├── football [799 entries] │ │ ├── formula1 [687 entries] │ │ ├── gymnastics [719 entries] │ │ ├── hockey [572 entries] │ │ ├── ice_hockey [715 entries] │ │ ├── kabaddi [454 entries] │ │ ├── motogp [679 entries] │ │ ├── shooting [536 entries] │ │ ├── swimming [689 entries] │ │ ├── table_tennis [713 entries] │ │ ├── tennis [718 entries] │ │ ├── volleyball [713 entries] │ │ ├── weight_lifting [577 entries] │ │ ├── wrestling [611 entries] │ │ ├── wwe [671 entries] . Setup . The repo contains the data, Colab Jupyter notebooks and trained weights for different training regimes. . Results . . . . . . . .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/video%20classification/2020/05/21/videoclassification.html",
            "relUrl": "/ai/ml/video%20classification/2020/05/21/videoclassification.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Multi-label Image Classification",
            "content": "Overview . This is a tutorial from Analytics Vidhya . The dataset of images contain more than two categories ie it is not a simple either/or . Each image in the dataset can contain only one category . Example: A dataset containing images such as dog, cat, rabbit, parrot Each image contains only dog, cat, parrot rabbit . . The above is know as multi-label image classification. . Question: Can we predict the genre of a movie by looking at the movie poster? And ofcourse a movie can belong to more than one genre. . The key is in the output layer - use a sigmoid activation instead of softmax. With Softmax as the probablity of one increases the probability of the other classses decrease (becuase the sum must equal 1). With Sigmoid however the probabilities are independent of each other. So with sigmoid the architecture will internally create N models where N is the number of classes. Cool huh?! . Setup . For details of the model and data see repo . Note: No attempt has been made to finetune the architecture and reduce the amount of overfitting and hence get a better training/validation loss. . Results . . . .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/multi-label%20image%20classification/2020/05/19/multilabelimageclass.html",
            "relUrl": "/ai/ml/multi-label%20image%20classification/2020/05/19/multilabelimageclass.html",
            "date": " • May 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Style Transfer using Neural Networks",
            "content": "Overview . This method renders the contents of a Original image in the style of a Reference image. For example in an image of a cityscape the contents can be considered as the hard edges of the buildings. The Reference image can contain lots of swirly patters and colours which will be know as the style of the image. The loss function is defined in parameter space as the difference between the content and style of the images as laid out below: . Loss = distance((style(reference_image) - style(generated_image)) + (distance(content(original_image) - content(generated_image)) . As a workflow exercise I attempted to render the image of an F1 car in the style of Picasso! I used images of recent Racing Point and Renault F1 cars. . Setup . The notebook, models and data are contained here . I trained it on Google Colab using both Tensorflow and Pytorch (separately ofcourse!) The trained model is implemented using a webapp. The user is asked to select an image and the analysis is displayed. . . Results . . Loss = distance((style(reference_image) - style(generated_image)) + (distance(content(original_image) - content(generated_image)) original_image = a picture of an F1 car reference_image = a style image such as Picasso . . .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/style%20transfer/2020/05/18/neuralstyletransfer.html",
            "relUrl": "/ai/ml/style%20transfer/2020/05/18/neuralstyletransfer.html",
            "date": " • May 18, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Predict F1 Car Constructor from Image",
            "content": "Overview . This is an exercise based on Formula 1 cars and is aimed at predicting which team a car belongs to by analysing an image. The image can be from any angle. This is an end-to-end example ir from gathering the data, training a model and deploying the trained model using a webapp on a local server. . Setup . The training setup, data are in this repo . This is a good example of a complete workflow . collecting data using google web search (local) | cleaning the data (removing duplicate images and other rubbish) (local) | porting data to Google Colab - When i was doing this I did not have my Github a/c organised properly Otherwise I would have done git init and ported the local repo to github From Google Colab it is easy to clone the Github repo | Once the data is in repo start a new jupyter notebook and select a GPU for training. | Download the trained model to the local dir | Modify the webapp to point to the trained model and do the predictions | . Note: I only used about 75 images per class (10 classes) to cut down on the training time. . I used Fastai’s recommended Starlette web api and with a bit of playing around with the css and html file got something suitable. . This is only meant for demo purposes and to motivate me to carry on with the ret of the course. . Results . . . Suggested Improvements: . Add more training data | Clean training data | Overfit and then play around with hyperparameters | Give webapp better UI | . . .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/image%20classification/2020/05/18/f1carspredictor.html",
            "relUrl": "/ai/ml/image%20classification/2020/05/18/f1carspredictor.html",
            "date": " • May 18, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Stochastic Gradient Descent",
            "content": "Overview . ML is basically this - you make a prediction, see how far that prediction is from the truth, fine-tune your prediction and keep going until the difference between between the prediction and the actual reaches an acceptable level. If we imagine the loss function plotted as a U curve then we are trying to reach the bottom of the curve as quickly as possible ie minimise the loss. From our starting point, a guess, we then take a small step in the direction of lower loss. The small step is known as the learning rate and the direction of lower loss is worked out from the slope of the curve. . Minimising the loss . Below is a regression example. The pink dots are the initial set of data. To this we have to find a general approximation that will satisfy any new points. Our initial guess is the black line. . We then go through the fllowing sequence: Loss = New Value - Actual Value Find slope of the loss function Move down the loss curve by a small amount (the Learning Rate) Find the new loss . . Pytorch implementation . y is the actual y_hat is the prediction (based on a set of weights a) loss is the mean squared error between y_hat and y def mse(y_hat,y): return((y_hat-y)**2).mean() def update(): y_hat = x@a loss=mse(y_hat,y) loss.backward() with torch.no_grad(): a.sub_(lr * a.grad) a.grad.zero_() . References . Fast.ai | Machine Learning Mastery | D :bowtie: — .",
            "url": "https://onpointai.github.io/onpointai/ai/ml%20gradient%20descent/2020/05/17/sgd.html",
            "relUrl": "/ai/ml%20gradient%20descent/2020/05/17/sgd.html",
            "date": " • May 17, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Overview of AI/ML",
            "content": "The contents of this page should be treated as a set of flashcards with each giving some insight into the definition, structure and scope of AI tools in everyday use. . Note: Reinforcement Learning (think Apha Go) has no practical applications at the moment for me and will therefore not be discussed. . . . . . . . . . . . It has been shown that the winner is not necessarily the smartest or the one with the best computer. Instead it is the person who can develop their ideas quickly. . . .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/visuals/2020/05/15/overview.html",
            "relUrl": "/ai/ml/visuals/2020/05/15/overview.html",
            "date": " • May 15, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Reference Material",
            "content": "This is a list of links to articles, blogposts, videos, tutorials that I have found very useful on my ML journey. It serves as a useful reference point and is ever-growing. . Setup . Tensorflow or Pytorch? . Google Colab . I love articles, tutorials, videos by the following: . Adrian Rosebrock Pyimagesearch. Andrej Karpathy blog. Chris Olah blog. Andrew Ng Coursera. Francois Chollet Book. Jeremey Howard Fast.ai. Rachel Thomas Fast.ai. Chris Albon Great flashcards - Buy them!. Aakash Nain Jovian. Hannah Fry [Makes number fun!]http://www.hannahfry.co.uk(). Jason Brownlee Tutorials. . Powerful Quotes . F Chollet: “You don’t need to know everything. You don’t really need a formal background in this or that – though it helps, you don’t even need a PhD. You do, however, need to be constantly learning, be curious, read books. Don’t be “too busy” to learn, or otherwise proud of your ignorance.” “Honestly, the question is not, and has never been, “ can ML replace radiologists/etc” (which won’t happen in the foreseeable future). The question is, how can radiology/etc utilise ML to improve outcomes, decrease the cost of car, and broaden accessibility.” . | Geoff Hinton: “Read enough sp you start developing intuitions and then trust your intuitions and go for it!.” . | Andrew Ng: “Deep Learning is a superpower. With it you can make a computer see, synthesize novel art, translate languages, render a medical diagnosis, or build pieces of a car that can drive itself. If that isn’t a superpower, I don’t know what is.” . | . . . Ignore stuff below. . . You can include alert boxes …and… . . You can include info boxes Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Tables . | Tensorflow | Pytorch | |-|-| | Keras | Fastai | — .",
            "url": "https://onpointai.github.io/onpointai/markdown/2020/05/14/referencematerial.html",
            "relUrl": "/markdown/2020/05/14/referencematerial.html",
            "date": " • May 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Dexter D’Silva . I am an Aeronautical Engineer having worked at: Airbus Wing Shape Team (High Speed Aerodynamics). EADS Innovation Works. Racing Point Formula 1 team (Aero Design processes). . I was always interested in writing macros, developing software on a one-off basis and saw myself writing advanced macros for CATIA automation initially in the field of wing shaping. . In 2017 I came across this video by Andrew Ng and since then I have been hooked on AI/Machine Learning and want to learn as much as I can about it with a view to applying my own spin on implementing flavours of it in the work that I do. . I am pretty good at connecting the dots of technology in the field of aero design where I work with CAD and CFD. I am always working on tutorials to build intuition about AI and explore it’s feasibility in my daily work, where my domain knowledge will help me to enhance my value to my customer/employer. . The other area of interest is exploring the use of AI to analyse the huge amount of data generated by CFD, wind tunnel testing and on-track or flight testing and building tools to gain easy access to knowledge, to decipher, disseminate and democratise within the organisation. . Here is my CV . Online courses with certificates . Coursera/Stanford | Deep Learning - AI for Everyone | Zero to Deep Learning | Deep Learning in Python | Supervised Learning - Scikit Learn | NLP - Python | Statistical Thinking in Python | Unsupervised Learning with Python | Intermediate Python for Data Science | Introduction Python for Data Science | Pandas Foundation | . Other Online Courses . Fast.ai | . Inspired by Fast.ai . . :cowboy_hat_face: . . .",
          "url": "https://onpointai.github.io/onpointai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://onpointai.github.io/onpointai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}